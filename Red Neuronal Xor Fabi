#Flores Diaz Fabiola
#Perceptron XOR

import numpy as np 

def sigmoid(x):
    # Función de activación sigmoidal
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    # Derivada de la función de activación sigmoidal
    return sigmoid(x) * (1 - sigmoid(x))

training_data = np.array([[0,0],[0,1],[1,0],[1,1]])
target_data = np.array([[0],[1],[1],[0]])

# Definimos los pesos de la red neuronal
w1 = np.random.randn(2, 16)
w2 = np.random.randn(16, 1)

# Definimos los hiperparámetros
lr = 0.1
epochs = 10000

# Entrenamos el modelo
for i in range(epochs):
    # Forward pass
    hidden_layer = sigmoid(np.dot(training_data, w1))
    output_layer = sigmoid(np.dot(hidden_layer, w2))
    
    # Backward pass
    error = target_data - output_layer
    d_output = error * sigmoid_deriv(output_layer)
    error_hidden = d_output.dot(w2.T)
    d_hidden = error_hidden * sigmoid_deriv(hidden_layer)
    
    # Actualizamos los pesos
    w2 += hidden_layer.T.dot(d_output) * lr
    w1 += training_data.T.dot(d_hidden) * lr
    
# Hacemos una predicción con los datos de entrenamiento
hidden_layer = sigmoid(np.dot(training_data, w1))
output_layer = sigmoid(np.dot(hidden_layer, w2))
prediction = np.round(output_layer)

# Evaluamos la precisión del modelo
accuracy = np.mean(prediction == target_data) * 100
print("Precisión: {}%".format(accuracy))
print(prediction)

